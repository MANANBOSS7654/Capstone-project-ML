{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to analyze the Netflix Dataset of movies and TV shows until 2019, sourced from the third-party search engine Flixable. The goal is to group the content into relevant clusters using NLP techniques to improve the user experience through a recommendation system. This will help prevent subscriber churn for Netflix, which currently has over 220 million subscribers.\n",
        "\n",
        "Additionally, the dataset will be analyzed to uncover insights and trends in the streaming entertainment industry.\n",
        "\n",
        "The project followed a step-by-step process:\n",
        "\n",
        "Handling null values in the dataset.\n",
        "Managing nested columns (director, cast, listed_in, country) for better visualization.\n",
        "Binning the rating attribute into categories (adult, children's, family-friendly, not rated).\n",
        "Performing Exploratory Data Analysis (EDA) to gain insights for preventing subscriber churn.\n",
        "Creating clusters using attributes like director, cast, country, genre, rating, and description. These attributes were tokenized, preprocessed, and vectorized using TF-IDF vectorizer.\n",
        "Reducing the dimensionality of the dataset using PCA to improve performance.\n",
        "Employing K-Means Clustering and Agglomerative Hierarchical Clustering algorithms, determining optimal cluster numbers (4 for K-Means, 2 for hierarchical clustering) through various evaluation methods.\n",
        "Developing a content-based recommender system using cosine similarity matrix to provide personalized recommendations to users and reduce subscriber churn for Netflix. \\\n",
        "This comprehensive analysis and recommendation system are expected to enhance user satisfaction, leading to improved retention rates for Netflix."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Context\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "In this project, you are required to do\n",
        "\n",
        "Exploratory Data Analysis\n",
        "Understanding what type content is available in different countries\n",
        "If Netflix has been increasingly focusing on TV rather than movies in recent years.\n",
        "Clustering similar content by matching text-based features"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# libraries used to process textual data\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")"
      ],
      "metadata": {
        "id": "VkZbxL4-r2hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Rows and Column count in the Dataset: Rows= {df.shape[0]}, Columns= {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count\n",
        "print(f\"The total number of duplicated observations in the dataset: {df.duplicated().sum()}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Percentage of null values in each category\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(df.isnull(), cbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,8))\n",
        "plots= sns.barplot(x=df.columns,y=df.isna().sum())\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width() / 2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0, 8),\n",
        "                     textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3vOrEDjvUA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The dataset \"Netflix Movies and TV Shows Clustering\" comprises 12 columns, with only one column having an integer data type. It does not contain any duplicate values, but it does have null values in five columns: director, cast, country, date_added, and rating.\n",
        "\n",
        "This dataset provides a valuable resource for exploring trends in the range of movies and TV shows available on Netflix. Additionally, it can be utilized for developing clustering models to categorize similar titles together based on shared attributes such as genre, country of origin, and rating."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f\"Available columns:\\n{df.columns.to_list()}\")\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable description of the Netflix Movies and TV Shows Clustering Dataset is as follows:\n",
        "\n",
        "show_id: Unique identifier for each movie/show.\n",
        "\n",
        "type: Indicates whether the entry is a movie or a TV show.\n",
        "\n",
        "title: Name of the movie or TV show.\n",
        "\n",
        "director: Name of the director(s) of the movie or TV show.\n",
        "\n",
        "cast: Names of the actors and actresses featured in the movie or TV show.\n",
        "\n",
        "country: Country or countries where the movie or TV show was produced.\n",
        "\n",
        "date_added: Date when the movie or TV show was added to Netflix.\n",
        "\n",
        "release_year: Year when the movie or TV show was released.\n",
        "\n",
        "rating: TV rating or movie rating of the movie or TV show.\n",
        "\n",
        "duration: Length of the movie or TV show in minutes or seasons.\n",
        "\n",
        "listed_in: Categories or genres of the movie or TV show.\n",
        "\n",
        "description: Brief synopsis or summary of the movie or TV show."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(f\"The number of unique values in: \")\n",
        "print(\"-\"*35)\n",
        "for i in df.columns:\n",
        "  print(f\"'{i}' : {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handling Null values from each feature"
      ],
      "metadata": {
        "id": "8A_CbwSqv7wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Let's find out the percentage of null values in each category in order to deal with it.\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"date_added\"].value_counts()"
      ],
      "metadata": {
        "id": "_aT14qHf4y50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "n82JybPt40Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['country'].value_counts()"
      ],
      "metadata": {
        "id": "W-eXYJMe43Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Imputing null value as per our discussion\n",
        "# imputing with unknown in null values of director and cast feature\n",
        "df[['director','cast']]=df[['director','cast']].fillna(\"Unknown\")\n",
        "\n",
        "# Imputing null values of country with Mode\n",
        "df['country']=df['country'].fillna(df['country'].mode()[0])\n",
        "\n",
        "# Dropping remaining null values of date_added and rating\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "H53nEtWD48y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Rechecking the Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Rechecking the percentage of null values in each category\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "7EquCY9V4-35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKj3WbXc5CGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Handling nested columns i.e 'director', 'cast', 'listed_in' and 'country'"
      ],
      "metadata": {
        "id": "G4x8s6KBv9BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's create a copy of dataframe and unnest the original one\n",
        "df_new= df.copy()\n"
      ],
      "metadata": {
        "id": "aS3180s_wAKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unnesting 'Directors' column\n",
        "dir_constraint=df['director'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df1 = pd.DataFrame(dir_constraint, index = df['title'])\n",
        "df1 = df1.stack()\n",
        "df1 = pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "dDlUibD0wErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unnesting 'cast' column\n",
        "cast_constraint=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df2 = pd.DataFrame(cast_constraint, index = df['title'])\n",
        "df2 = df2.stack()\n",
        "df2 = pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "6WiDz41wwHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unnesting 'listed_in' column\n",
        "listed_constraint=df['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(listed_constraint, index = df['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)\n"
      ],
      "metadata": {
        "id": "L0iWmc-PwK0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unnesting 'country' column\n",
        "country_constraint=df['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(country_constraint, index = df['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "xV-QKY2uwNS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all the unnested dataframes"
      ],
      "metadata": {
        "id": "0hroJBWlwQ1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of (director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# Head of final merged dataframe\n",
        "df7.head()"
      ],
      "metadata": {
        "id": "U5fC0pB-wR4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Merging unnested data with the created dataframe in order to make the final dataframe\n",
        "df = df7.merge(df[['type', 'title', 'date_added', 'release_year', 'rating', 'duration','description']],on=['title'],how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5F8FQd9AwXg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Typecasting of attributes"
      ],
      "metadata": {
        "id": "JQ4KZKcGwaxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking info of the dataset before typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "r8Udb5MmwbLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Typecasting duration into integer by removing 'min' and 'season' from the end\n",
        "df['duration'] = df['duration'].apply(lambda x: int(x.split()[0]) if isinstance(x, str) else x)\n",
        "\n",
        "# Typecasting string object to datetime object of date_added column\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], format='mixed', errors='coerce')\n",
        "\n",
        "# Extracting date, day, month and year from date_added column\n",
        "df[\"day_added\"] = df[\"date_added\"].dt.day\n",
        "df[\"month_added\"] = df[\"date_added\"].dt.month\n",
        "df[\"year_added\"] = df[\"date_added\"].dt.year\n",
        "\n",
        "# Dropping date_added\n",
        "df.drop('date_added', axis=1, inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DIlZWdJa7C-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Kcd9YL-iwr05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Binning of Rating attribute\n",
        "In rating columns we have different categories these are content rating classifications that are commonly used in the United States and other countries to indicate the appropriateness of media content for different age groups. Let's understand each of them and binnig them accordingly:\n",
        "\n",
        "TV-MA: This rating is used for mature audiences only, and it may contain strong language, violence, nudity, and sexual content.\n",
        "\n",
        "R: This rating is used for movies that are intended for audiences 17 and older. It may contain graphic violence, strong language, drug use, and sexual content.\n",
        "\n",
        "PG-13: This rating is used for movies that may not be suitable for children under 13. It may contain violence, mild to moderate language, and suggestive content.\n",
        "\n",
        "TV-14: This rating is used for TV shows that may not be suitable for children under 14. It may contain violence, strong language, sexual situations, and suggestive dialogue.\n",
        "\n",
        "TV-PG: This rating is used for TV shows that may not be suitable for children under 8. It may contain mild violence, language, and suggestive content.\n",
        "\n",
        "NR: This stands for \"Not Rated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.\n",
        "\n",
        "TV-G: This rating is used for TV shows that are suitable for all ages. It may contain some mild violence, language, and suggestive content.\n",
        "\n",
        "TV-Y: This rating is used for children's TV shows that are suitable for all ages. It is intended to be appropriate for preschool children.\n",
        "\n",
        "TV-Y7: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain mild violence and scary content.\n",
        "\n",
        "PG: This rating is used for movies that may not be suitable for children under 10. It may contain mild language, some violence, and some suggestive content.\n",
        "\n",
        "G: This rating is used for movies that are suitable for general audiences. It may contain some mild language and some violence.\n",
        "\n",
        "NC-17: This rating is used for movies that are intended for adults only. It may contain explicit sexual content, violence, and language.\n",
        "\n",
        "TV-Y7-FV: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain fantasy violence.\n",
        "\n",
        "UR: This stands for \"Unrated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.\n",
        "\n",
        "Let's not complicate it and create bins as following:\n",
        "\n",
        "Adult Content: TV-MA, NC-17, R\n",
        "Children Content: TV-PG, PG, TV-G, G\n",
        "Teen Content: PG-13, TV-14\n",
        "Family-friendly Content: TV-Y, TV-Y7, TV-Y7-FV\n",
        "Not Rated: NR, UR"
      ],
      "metadata": {
        "id": "kJKNXS_ixWvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Binning the values in the rating column\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-friendly Content',\n",
        "              'UR':'Not Rated'}\n",
        "\n",
        "df['rating'].replace(rating_map, inplace = True)\n",
        "df['rating'].unique()"
      ],
      "metadata": {
        "id": "-eoQwLm2xXXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking head after binning\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-mKV2nIwxahM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Separating Movies and TV Shows"
      ],
      "metadata": {
        "id": "LiJc8C8uxede"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Spearating the dataframes for further analysis\n",
        "df_movies= df[df['type']== 'Movie']\n",
        "df_tvshows= df[df['type']== 'TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(df_movies.shape, df_tvshows.shape)"
      ],
      "metadata": {
        "id": "QcR6OC1cxe8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have divided data wrangling into five different sections:\n",
        "\n",
        "In this section we have imputed/drop the null values of:\n",
        "Imputed 'director' and 'cast' with 'Unknown'.\n",
        "Imputed 'country' with Mode.\n",
        "Drop null values of 'date_added' and 'rating' (less percentage).\n",
        "We have unnested values from following features:\n",
        "\n",
        "'director'\n",
        "'cast'\n",
        "'listed_in'\n",
        "'country'\n",
        "We have unnested the values and stored in different dataframes and then merged all the dataframe with the original one using left join in order to get the isolated values of each of the feature. 3.* We have typecasted the following features:\n",
        "\n",
        "'duration' into integer (Removing min and seasons from the values).\n",
        "'date_added' to datetime (Into the required format).\n",
        "We have also extracted the following features:\n",
        "'date' from 'date_added'.\n",
        "'month' from 'date_added'.\n",
        "'year' from 'date_added'.\n",
        "We have seen that the 'rating' column contains various coded categories, so we have decided to create 5 bins and distribute the values accordingly:\n",
        "Adult: TV-MA, NC-17\n",
        "Restricted: R, UR\n",
        "Teen: PG-13, TV-14\n",
        "All Ages: TV-G, TV-Y, TV-Y7, TV-Y7-FV, PG, G, TV-PG\n",
        "Not Rated: NR\n",
        "Lastly we have splitted the dataframe into two df one is 'df_movies' that contains only Movies and the other is 'df_tvshows' that contains only TV Shows for our further analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chart - 1 Chart - 1 (The relative percentage of total number of Movies and TV Shows over Netflix?)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Chart - 1 visualization code\n",
        "\n",
        "labels = ['TV Show', 'Movie']\n",
        "values = [df.type.value_counts()[1], df.type.value_counts()[0]]\n",
        "\n",
        "# Colors\n",
        "colors = ['#ffd700', '#008000']\n",
        "\n",
        "# Create pie chart\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title_text='Type of Content Watched on Netflix',\n",
        "    title_x=0.5,\n",
        "    height=500,\n",
        "    width=500,\n",
        "    legend=dict(x=0.9),\n",
        "    annotations=[dict(text='Type of Content', font_size=20, showarrow=False)]\n",
        ")\n",
        "\n",
        "# Set colors\n",
        "fig.update_traces(marker=dict(colors=colors))"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the percent of TV shows and movie data present on Netflix Data set"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the majority of the content on Netflix is movies, which account for around two-thirds of the total content. TV shows make up the remaining one-third of the content.\n",
        "\n",
        "we can conclude that in the given Data set only 28.3% are TV Shows and 71.7% are Movies."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes! the production house should more focus on quality movies because there is high competition in the market.\n",
        "\n",
        "TV Shows are less in numbers hence good opportunity for business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 2 (How content is distributed over Netflix?)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "for i,j,k in ((df, 'Overall',0),(df_movies, 'Movies',1),(df_tvshows, 'TV Shows',2)):\n",
        "  plt.subplot(1,3,k+1)\n",
        "  count= i['rating'].value_counts()\n",
        "  plt.pie(count, labels=count.index,explode=(0,0,0,0,0.5),colors=['orangered','dodgerblue','lightgreen','mediumslateblue','yellow'],\n",
        "          autopct='%1.1f%%', labeldistance=1.1,wedgeprops={\"edgecolor\" : \"black\",'linewidth': 1,'antialiased': True})\n",
        "  plt.title(f\"Distribution of Content Rating on Netflix '{j}'\")\n",
        "  plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chosen this chart to know the percentage of type of content present in the Netflix."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.We found that most of the content present in the Netflix belongs to Adult and the teen categories.\n",
        "\n",
        "2.Another important insight we can see that Family friendly content less in Movies compared to TV Shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.For high gains production house should more focus on Teen and Adult content.\n",
        "\n",
        "2.There is good chances of growth in Family-friendly category in TV Shows"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 3 (Who are the top actors performing in Movies and TV Shows?)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Top 10 casts in Movies and TV Shows\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_actor = i.groupby(['Actors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Actors\",x = 'title', data = df_actor, palette='Set1')\n",
        "  plt.title(f'Actors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which actors are more popular on Netflix."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found an interesting insight that most of the Actors in Movies are from INDIA.\n",
        "\n",
        "No popular actors from india in TV Shows."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indians are movie lover, they love to watch movies hence business should target indian audience for Movies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 4 (Who are the top Directors directing Movies and TV Shows?)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Top 10 Directors in Movies and TV Shows\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_director = i.groupby(['Directors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Directors\",x = 'title', data = df_director, palette='Paired')\n",
        "  plt.title(f'Directors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which director is popular in Movies and which one is popular in TV Shows."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that most of the movies directed by jan suter.\n",
        "\n",
        "Most TV shows directed by ken burns."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Movie/tvshows producers can select the popular director for their upcoming projects"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 5 (What are the top 10 Countries involved in content creation?)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "df_country = df.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plt.figure(figsize=(15,6))\n",
        "plots= sns.barplot(y = \"Country\",x = 'title', data = df_country)\n",
        "plt.xticks(rotation = 60)\n",
        "plt.title('Top 10 Countries for content creation')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces Maximum number of TV Shows and Movies."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States is the top country producing both movies and TV shows on Netflix. This suggests that Netflix is heavily influenced by American content.\n",
        "\n",
        "India is the second-highest producer of movies on Netflix, indicating the growing popularity of Bollywood movies worldwide.\n",
        "\n",
        "Country like canada, france, japan also have significant presence in the data set showing diversity of content on the netflix."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can have a positive impact on Netflix's business by highlighting opportunities for growth and expansion, such as investing in American and Bollywood content, and acquiring more diverse content."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 6 (Which Countries has the highest spread of Movies and TV Shows over Netflix)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Analysing top15 countries with most content\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "sns.countplot(x=df['Country'],order=df['Country'].value_counts().index[0:15],hue=df['type'],palette =\"Set1\")\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 countries with most contents', fontsize=15, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_country = i.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Country\",x = 'title', data = df_country, palette='Set1')\n",
        "  plt.title(f'Top 10 countries launching {j} back to back')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces which type of content the most."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INDIA Produces most amount of Movies in compare to TV Shows.\n",
        "\n",
        "Japan and South korea produces more TV Shows in compare to Movies."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained can have a positive impact on Netflix's business by highlighting opportunities for growth and expansion, such as acquiring and producing more movies from India and more TV shows from Japan and South Korea."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 7 (Which Genres are Popular in Netflix)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "df_genre = df.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre)\n",
        "plt.title(f'Most popular genre on Netflix')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_genre = i.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre, palette='Set1')\n",
        "  plt.title(f'Most popular genre of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "  plt.yticks(rotation = 45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph tells us which genre is most popular in Netflix."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International movies genre is most popular in both the TV Shows and Movies category. Followed by Drama and comedy."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained can have a positive impact on Netflix's business by helping the platform understand what genres and types of content are popular with its audience. This information can help Netflix tailor its content acquisition and production strategies to better cater to the preferences of its viewers, which can lead to increased engagement and customer satisfaction."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "TfPHIyf826D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 8 (Total number of Movies/TV Shows released and added per year on Netflix?)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(20,6))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_release_year = i.groupby(['release_year']).agg({'title':'nunique'}).reset_index().sort_values(by=['release_year'],ascending=False)[:14]\n",
        "  plots= sns.barplot(x = 'release_year',y= 'title', data = df_release_year, palette='husl')\n",
        "  plt.title(f'{j} released by year')\n",
        "  plt.ylabel(f\"Number of {j} released\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_country = i.groupby(['year_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'year_added',y= 'title', data = df_country, palette='husl')\n",
        "  plt.title(f'{j} added to Netflix by year')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us how many movies and TV Show released and added in a year on Netflix."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of movies and TV shows added on Netflix has been increasing steadily every year. But since 2018, the number of Movies released on Netflix has been lowered and the number of TV shows released has been significantly increased. In terms of movies and TV Shows addition, in 2020 Number of movies added as compared to 2019 were vey less and on the other side number of TV Shows were more as compare to 2019."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that the number of movies added has decreased since 2018 while the number of TV shows added has significantly increased could potentially lead to negative growth for Netflix. This could be due to various reasons such as changing consumer preferences, increased competition from other streaming services, and higher production costs associated with creating movies.\n",
        "\n",
        "To mitigate the potential negative impact, Netflix could explore strategies to diversify its content offerings and adapt to changing consumer preferences. This could include investing in a mix of movies, TV shows, and other forms of original content such as documentaries, limited series, and stand-up comedy specials. By diversifying its content offerings, Netflix can attract a wider audience and maintain its relevance in the ever-evolving streaming landscape."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 9 (Total Number of Movies/TV Shows added per month on Netflix)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_month = i.groupby(['month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['month_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'month_added',y='title', data = df_month, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by month')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have plotted this graph to know in which month the movie/tv shows added is maximum and in which year minimum."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that October, November and December are the most popular months for TV shows addition.\n",
        "\n",
        "January, October and December are the most popular months for movie addition.\n",
        "\n",
        "February is the least popular month for the movies and TV shows to be added on Netflix."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained can help Netflix create a positive business impact by identifying the most popular months for new content additions. This can help Netflix plan content releases during peak periods, leading to increased user engagement and retention.\n",
        "\n",
        "The insight that February is the least popular month for new content additions could potentially lead to negative growth if Netflix does not maintain a consistent flow of new content during this period. It is important for Netflix to keep its audience engaged throughout the year to avoid dissatisfaction and potential loss of subscribers."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 10 (Total Number of Movies/TV Shows added per day on Netflix)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_day = i.groupby(['day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['day_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'day_added',y='title', data = df_day, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by day')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='bottom',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points', rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the day when most of the movies added in a month."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plots, it can be observed that most of the movies and TV shows are added at the beginning or middle of the month. It could be because most people tend to have more free time at the beginning of the month after getting paid, and releasing new content during that time could increase viewership. By releasing new content at the beginning and middle of the month, subscribers are more likely to feel that they are getting value for their money, which could lead to increased retention rates."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, releasing new content at regular intervals helps to keep users engaged with the platform, as they will have something new to look forward to every few weeks. This can lead to increased viewing hours and user satisfaction, both of which can have positive impacts on the business."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 11 (What is the Month-wise number of content added in each year on Netflix?)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x = 'year_added',y='title', data = df_year_month, palette = 'hls', hue=df_year_month['month_added'], marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bivariate graph helps us in knowing which month is dominating in adding movie/tvshows in a year."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is no specific trend is followed, Instead of this some consecutive years shows month wise trend.\n",
        "\n",
        "From 2008 to 2009 we see movies added in the month of February, and from 2009 to 2011 movies added in the month of February and October.\n",
        "\n",
        "After 2015 majority content added in the month of october to december"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Producers should add there movies in the month when audience is more responsive.\n",
        "\n",
        "Although no specific trend is shown but most movies should be uploaded in year end with some discount in the subscription."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 12 (What is the Day-wise number of content added in each year on Netflix?)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x = 'year_added',y='title', data = df_year_month, palette = 'hls', hue=df_year_month['day_added'], marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph help us in knowing which day is more frequent in movie addition year wise."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies from 2008 to 2009 added on 5th day of the month.\n",
        "Movies from 2009 to 2010 added on 15th day of the month.\n",
        "Most of the movies from 2010 to 2012 added in the month end.\n",
        "From 2015 onwords most of the movies are added in month end or mid month."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently most of the movies are added in 15th day of month or at the last day of month, so before releasing the movies consider this trend also."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart - 13 (What is the Distribution of Duration of contents over Netflix?)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(10,7))\n",
        "plots= sns.distplot(df_movies['duration'],kde=False, color=['green'])\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "for bar in plots.patches:\n",
        "   plots.annotate(bar.get_height(),\n",
        "                  (bar.get_x() + bar.get_width() / 2,\n",
        "                   bar.get_height()), ha='center', va='bottom',\n",
        "                  size=7, xytext=(0, 5),\n",
        "                  textcoords='offset points', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "df_duration = df_tvshows.groupby(['duration']).agg({'title':'nunique'}).reset_index().sort_values(by=['duration'],ascending=False)\n",
        "plots= sns.barplot(x = 'duration',y='title', data = df_duration, palette='husl')\n",
        "plt.title(f'Barplot of TV Shows Duration')\n",
        "plt.ylabel(f\"Content count\")\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "for bar in plots.patches:\n",
        "   plots.annotate(bar.get_height(),\n",
        "                  (bar.get_x() + bar.get_width() / 2,\n",
        "                   bar.get_height()), ha='center', va='bottom',\n",
        "                  size=12, xytext=(0, 8),\n",
        "                  textcoords='offset points', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgmqDDEi83bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the duration distribution for Movies and TV Shows on Netflix."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The histogram of the distribution of movie durations in minutes on Netflix shows that the **majority of movies on Netflix have a duration between 80 to 120 minutes. **\n",
        "\n",
        "2.The countplot of the distribution of TV show durations in seasons on Netflix shows that the most common duration for TV shows on Netflix is one season, followed by two seasons."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is already loaded\n",
        "# Adding a column for counting occurrences\n",
        "df['count'] = 1\n",
        "\n",
        "# Get the top 10 countries with the highest number of movies/TV shows\n",
        "data = df.groupby('Country')['count'].sum().sort_values(ascending=False).head(10).index\n",
        "df_heatmap = df[df['Country'].isin(data)]\n",
        "\n",
        "# Create a crosstab of the normalized count per country and rating\n",
        "df_heatmap = pd.crosstab(df_heatmap['Country'], df_heatmap['rating'], normalize=\"index\").T\n",
        "\n",
        "# Plotting the heatmap\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Defining the order of countries and ratings\n",
        "country_order = ['United States', 'India', 'United Kingdom', 'Canada', 'France', 'Japan', 'Spain', 'South Korea', 'Germany', 'Mexico']\n",
        "rating_order = ['Adult Content', 'Teen Content', 'Children Content', 'Family-friendly Content', 'Not Rated']\n",
        "\n",
        "# Plotting the heatmap with the specified order\n",
        "sns.heatmap(df_heatmap.loc[rating_order, country_order], cmap=\"jet\", square=True, linewidth=2.5, cbar=False, annot=True, fmt='1.0%',\n",
        "            vmax=.6, vmin=0.05, ax=ax, annot_kws={\"fontsize\": 12})\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mNGAywOT_CDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us which countries producing which type of content the most."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that most of the countries produces content related to Adult and Teen.\n",
        "\n",
        "Amomg all the countries INDIA has less content in Adult segment than teen content.\n",
        "\n",
        "85% of content is Adult content from spain.\n",
        "\n",
        "Canada produces more content related to Children and Family-Friendly content."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1:\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Hypothetical Statement 2:\n",
        "\n",
        "Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Hypothetical Statement 3:\n",
        "\n",
        "Null Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "Alternative Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Subset the data to only include drama and comedy movies\n",
        "subset = df[df['Genre'].str.contains('Dramas') | df['Genre'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['Genre'].str.contains('Dramas')]) / len(subset)\n",
        "comedy_prop = len(subset[subset['Genre'].str.contains('Comedies')]) / len(subset)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test we have used to obtain the P-value is the z-test for proportions."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions was chosen because we are comparing the proportions of two categorical variables (drama movies and comedy movies) in a sample. The null hypothesis and alternative hypothesis are about the difference in proportions, and we want to determine if the observed difference in proportions is statistically significant or not. The z-test for proportions is appropriate for this situation because it allows us to compare two proportions and calculate the probability of observing the difference we see in our sample if the null hypothesis were true."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# To test this hypothesis, we perform a two-sample t-test.\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create separate dataframes for TV shows in 2020 and 2021\n",
        "tv_2020 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2020)]\n",
        "tv_2021 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2021)]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t, p = ttest_ind(tv_2020['duration'].astype(int),\n",
        "                 tv_2021['duration'].astype(int), equal_var=False)\n",
        "print('t-value: ', t)\n",
        "print('p-value: ', p)\n",
        "\n",
        "# Print the results\n",
        "if p < 0.05:\n",
        "    print('Reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.')\n",
        "else:\n",
        "    print('Failed to reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.')\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the P-Value is a two-sample t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of two different samples (TV shows added in 2020 vs TV shows added in 2021) to determine whether they are significantly different. Additionally, we assume that the two samples have unequal variances since it is unlikely that the duration of TV shows added in 2020 and 2021 would have the exact same variance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "Alternative Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "tv_proportion = np.sum(df_tvshows['Country'].str.contains('United States')) / len(df_tvshows)\n",
        "movie_proportion = np.sum(df_movies['Country'].str.contains('United States')) / len(df_movies)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(tv_proportion * len(df_tvshows)), int(movie_proportion * len(df_movies))]\n",
        "nobs = [len(df_tvshows), len(df_movies)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain P-Value is a two-sample proportion test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose this specific statistical test because it is appropriate for comparing two proportions, and it helps us to determine whether the difference between the two proportions is due to chance or not."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Since we have already dealed with null value. So it is not needed now.\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Storing the continous value feature in a separate list\n",
        "continous_value_feature= [\"release_year\",\"duration\",\"day_added\",\"month_added\",\"year_added\"]\n",
        "\n",
        "# checking outliers with the help of box plot for continous features\n",
        "plt.figure(figsize=(16,5))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(1, 5, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# Here we are taking the copied dataframe as the data having more number of observations resulted in ram exhaustion.\n",
        "df.shape, df_new.shape"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning of rating in new dataframe\n",
        "df_new['rating'].replace(rating_map, inplace = True)\n",
        "\n",
        "# Checking sample after binning\n",
        "df_new.sample(2)"
      ],
      "metadata": {
        "id": "oKVeA7IMARsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Creating new feature content_detail with the help of other textual attributes\n",
        "df_new[\"content_detail\"]= df_new[\"cast\"]+\" \"+df_new[\"director\"]+\" \"+df_new[\"listed_in\"]+\" \"+df_new[\"type\"]+\" \"+df_new[\"rating\"]+\" \"+df_new[\"country\"]+\" \"+df_new[\"description\"]\n",
        "\n",
        "#checking the manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Lower Casing\n",
        "df_new['content_detail']= df_new['content_detail'].str.lower()\n",
        "\n",
        "# Checking the manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# function to remove punctuations\n",
        "def remove_punctuations(text):\n",
        "    '''This function is used to remove the punctuations from the given sentence'''\n",
        "    #imorting needed library\n",
        "    import string\n",
        "    # replacing the punctuations with no space, which in effect deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped off punctuation marks\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "# Removing Punctuations from the content_detail\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_punctuations)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and Numbers from the given sentence'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # Replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text= re.sub(url_number_pattern,'', text)\n",
        "\n",
        "    # Replacing the digits with one space\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # return the text stripped off URL's and Numbers\n",
        "    return text\n",
        "\n",
        "\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_url_and_numbers)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "# Downloading stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# create a set of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# displaying stopwords\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_and_whitespaces(text):\n",
        "    '''This function is used for removing the stopwords from the given sentence'''\n",
        "    text = [word for word in text.split() if not word in stopwords.words('english')]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # removing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text\n",
        "\n",
        "\n",
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_stopwords_and_whitespaces)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "BC6frAvlA3k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['content_detail'][0]"
      ],
      "metadata": {
        "id": "ae2oZL_7BSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenizing the 'content_detail' column\n",
        "df_new['content_detail'] = df_new['content_detail'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Checking the 'content_detail' column for the 281st row\n",
        "df_new.iloc[281]['content_detail']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DUYPUwrYDFKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Importing WordNetLemmatizer from nltk module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lemmatizing_sentence(text):\n",
        "    '''This function is used for lemmatizing (changing the given word into meaningfull word) the words from the given sentence'''\n",
        "    text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text"
      ],
      "metadata": {
        "id": "gNegdt-ODPIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Rephrasing text by applying defined lemmatizing function\n",
        "df_new['content_detail']= df_new['content_detail'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "J9QUCWQEDSYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Lemmatization instead of Stemming for our project because:\n",
        "\n",
        "Lemmatization produces a more accurate base word: Unlike Stemming, which simply removes the suffix from a word, Lemmatization looks at the meaning of the word and its context to produce a more accurate base form.\n",
        "\n",
        "Lemmatization can handle different inflections: Lemmatization can handle various inflections of a word, including plural forms, verb tenses, and comparative forms, making it useful for natural language processing.\n",
        "\n",
        "Lemmatization produces real words: Lemmatization always produces a real word that can be found in a dictionary, making it easier to interpret the results of text analysis.\n",
        "\n",
        "Lemmatization improves text understanding: By reducing words to their base form, Lemmatization makes it easier to understand the context and meaning of a sentence.\n",
        "\n",
        "Lemmatization supports multiple languages: While Stemming may only work well for English, Lemmatization is effective for many different languages, making it a more versatile text processing technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "# Vectorizing Text\n",
        "# Importing needed libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=30000)        # Setting max features as 30000 to avoid RAM explosion\n",
        "\n",
        "\n",
        "# Fitting on TfidfVectorizer\n",
        "x= tfidfv.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Checking shape of the formed document matrix\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used TFIDF vectorization in place of BAG OF WORDS because Tf-idf vectorization takes into account the importance of each word in a document. TF-IDF also assigns higher values to rare words that are unique to a particular document, making them more important in the representation."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dimensionality Reduction\n",
        "# Importing PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Defining PCA object with desired number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fitting the PCA model\n",
        "pca.fit(x.toarray())\n",
        "\n",
        "# percent of variance captured by each component\n",
        "variance = pca.explained_variance_ratio_\n",
        "print(f\"Explained variance: {variance}\")"
      ],
      "metadata": {
        "id": "UnPEeFFJGtE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ploting the percent of variance captured versus the number of components in order to determine the reduced dimensions\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(variance)+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Number of Components')\n",
        "ax.set_ylabel('Percent of Variance Captured')\n",
        "ax.set_title('PCA Analysis')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X1P9vrWXHN5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In textual data processing, there are 30,000 attributes are created in text vectorization and this huge amount of columns cannot be dealed with our local machines. So, we will using the Principal Component Analysis(PCA) techniques to reduce the dimensions of this huge sparse matrix."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "-o0UxtnJHQGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering is a type of unsupervised machine learning algorithm used for partitioning a dataset into K clusters based on similarity of data points. The goal of the algorithm is to minimize the sum of squared distances between each data point and its corresponding cluster centroid. It works iteratively by assigning each data point to its nearest centroid and then re-computing the centroid of each cluster based on the new assignments. The algorithm terminates when the cluster assignments no longer change or when a maximum number of iterations is reached.\n",
        "\n",
        "Let's just itterate over a loop of 1 to 16 clusters and try to find the optimal number of clusters with ELBOW method."
      ],
      "metadata": {
        "id": "UMOjT2ZeHbl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it seems that the elbow is forming at the 2 clusters but before blindly believing it let's plot one more chart that itterates over the same number of cluters and determines the Silhouette Score at every point.\n",
        "\n",
        "Okay, but what is Silhouette Score?\n",
        "\n",
        "The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. It is used to evaluate the quality of clustering, where a higher score indicates that objects are more similar to their own cluster and dissimilar to other clusters.\n",
        "\n",
        "The silhouette score ranges from -1 to 1, where a score of 1 indicates that the object is well-matched to its own cluster, and poorly-matched to neighboring clusters. Conversely, a score of -1 indicates that the object is poorly-matched to its own cluster, and well-matched to neighboring clusters."
      ],
      "metadata": {
        "id": "5t9HPloeIH2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer( model,k=(2,16), metric='silhouette', timings=True, locate_elbow=False)\n",
        "\n",
        "\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()\n"
      ],
      "metadata": {
        "id": "aDua2glbIJrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer( model,k=(2,16), metric='silhouette', timings=True, locate_elbow=False)\n",
        "\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Gcw4Dzro4znS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Defining Range\n",
        "k_range = range(2, 7)\n",
        "for k in k_range:\n",
        "    Kmodel = KMeans(n_clusters=k)\n",
        "    labels = Kmodel.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "MhX3rfNFIMK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state= 0)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = kmeans.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9eq_r3I9IOU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[kmeans.labels_ == i, 2], x_transformed[kmeans.labels_ == i, 0], x_transformed[kmeans.labels_ == i, 1], c=colors[i])\n",
        "\n",
        "# Rotate the plot 30 degrees around the X axis and 45 degrees around the Z axis\n",
        "ax.view_init(elev=20, azim=-120)\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RV_3Ziq7IRMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add cluster values to the dateframe.\n",
        "df_new['kmeans_cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "acVEiH98ITYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    #Importing libraries\n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    df_wordcloud = df_new[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
        "for i in range(4):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxiAlmZ8Ia7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "#importing needed libraries\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# HIERARCHICAL CLUSTERING\n",
        "distances_linkage = linkage(x_transformed, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('All films/TV shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distances_linkage, no_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, but what is Dendogram and how to determine the optimal value of clusters?\n",
        "\n",
        "A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.\n",
        "From the above Dendogram we can say that optimal value of clusters is 2. But before assigning the vlaues to respective clusters, let's check the silhouette scores using Agglomerative clustering and follow the bottom up approach to aggregate the datapoints."
      ],
      "metadata": {
        "id": "DVxcUZ37Ii3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "jFeGsiyGIlzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-iEwyXJIoAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, linkage = 'ward') # Removing affinity parameter\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rOU_u_x_4Py7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2],c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XHchzbz_IqDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df_new['agglomerative_cluster'] = Agmodel.labels_\n"
      ],
      "metadata": {
        "id": "3FVSU7DoIsK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  #Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df_new[['agglomerative_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "b1fB0_hnIxrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BvxFFeYkIzYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Cosine similarity as it is a measure of similarity between two non-zero vectors in a multidimensional space. It measures the cosine of the angle between the two vectors, which ranges from -1 (opposite direction) to 1 (same direction), with 0 indicating orthogonality (the vectors are perpendicular to each other).\n",
        "\n",
        "In this project we have used cosine similarity which is used to determine how similar two documents or pieces of text are. We represent the documents as vectors in a high-dimensional space, where each dimension represents a word or term in the corpus. We can then calculate the cosine similarity between the vectors to determine how similar the documents are based on their word usage.\n",
        "\n",
        "We are using cosine similarity over tf-idf because:\n",
        "\n",
        "Cosine similarity handles high dimensional sparse data better.\n",
        "\n",
        "Cosine similarity captures the meaning of the text better than tf-idf. For example, if two items contain similar words but in different orders, cosine similarity would still consider them similar, while tf-idf may not. This is because tf-idf only considers the frequency of words in a document and not their order or meaning."
      ],
      "metadata": {
        "id": "1MPIttfrI2Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a TF-IDF vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=df_new):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "    return rec_table\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian movie\n",
        "recommend_content('Kal Ho Naa Ho')"
      ],
      "metadata": {
        "id": "-l5hRtEeI8Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testing non indian movie\n",
        "recommend_content('Zombieland')"
      ],
      "metadata": {
        "id": "eBjjhHQJI9zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testing indian tv show\n",
        "recommend_content('Zindagi Gulzar Hai')"
      ],
      "metadata": {
        "id": "VBmzX0OsI_wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testing non indian tv show\n",
        "recommend_content('Vampires')"
      ],
      "metadata": {
        "id": "HT2lG9dIJB5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen Silhoutte Score over Distortion Score (also known as inertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other clusters. It ranges from -1 to 1, with higher values indicating better cluster separation. A silhouette score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters. A score close to 0 indicates that the data point is on or very close to the boundary between two clusters. A score close to -1 indicates that the data point is probably assigned to the wrong cluster.\n",
        "\n",
        "The advantages of using silhouette score over distortion score are:\n",
        "\n",
        "Silhouette score takes into account both the cohesion (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only considers the compactness of each cluster.\n",
        "Silhouette score is less sensitive to the shape of the clusters, while distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "Silhouette score provides more intuitive and interpretable results, as it assigns a score to each data point rather than just a single value for the entire clustering solution."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have considered K-means as our final model, as we are getting the comparatevely high Silhoutte Score in K-means clustering and the resulted clusters are very well seperated from each others as have saw in the 3 dimensions.\n",
        "\n",
        "Also in some of the situations K-means works more accurately then other clustering methods such as:\n",
        "\n",
        "Speed: K-means is generally faster than hierarchical clustering, especially when dealing with large datasets, since it involves fewer calculations and iterations.\n",
        "\n",
        "Ease of use: K-means is relatively straightforward to implement and interpret, as it requires only a few parameters (such as the number of clusters) and produces a clear partitioning of the data.\n",
        "\n",
        "Scalability: K-means can easily handle datasets with a large number of variables or dimensions, whereas hierarchical clustering becomes computationally expensive as the number of data points and dimensions increase.\n",
        "\n",
        "Independence of clusters: K-means produces non-overlapping clusters, whereas hierarchical clustering can produce overlapping clusters or clusters that are nested within each other, which may not be ideal for certain applications."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusions drawn from EDA\n",
        "Based on the exploratory data analysis (EDA) of the Netflix movies and TV shows clustering dataset, we have drawn the following conclusions:\n",
        "\n",
        "Movies make up about two-thirds **of Netflix content, with TV shows comprising the remaining **one-third.\n",
        "\n",
        "Adult and teen categories are prevalent on Netflix, while family-friendly content is more common in TV shows than in movies.\n",
        "\n",
        "Indian actors dominate Netflix movies, while popular Indian actors are absent from TV shows.\n",
        "\n",
        "Jan Suter is the most common movie director, and Ken Burns is the most common TV show director on Netflix.\n",
        "\n",
        "The United States is the largest producer of movies and TV shows on Netflix, followed by India. Japan and South Korea have more TV shows than movies, indicating growth potential in that area.\n",
        "\n",
        "International movies, drama, and comedy are the most popular genres on Netflix.\n",
        "\n",
        "TV show additions on Netflix have increased since 2018, while movie additions have decreased. In 2020, fewer movies were added compared to 2019, but more TV shows were added.\n",
        "\n",
        "October, November, and December are popular months for adding TV shows, while January, October, and November are popular for adding movies. February sees the least additions.\n",
        "\n",
        "Movies and TV shows are typically added at the beginning or middle of the month and are popularly added on weekends.\n",
        "\n",
        "Most movies on Netflix have durations between 80 to 120 minutes, while TV shows commonly have one or two seasons.\n",
        "\n",
        "Various countries contribute adult and teen content, with Spain producing the most adult content and Canada focusing on children and family-friendly categories.\n",
        "\n",
        "Conclusions drawn from ML Model\n",
        "Implemented K-Means Clustering and Agglomerative Hierarchical Clustering, to cluster the Netflix Movies TV show dataset.\n",
        "The optimal number of clusters we are getting from K-means is 4, whereas for Agglomerative Hierarchical Clustering the optimal number of clusters are found out to be 2.\n",
        "We chose Silhouette Score as the evaluation metric over distortion score because it provides a more intuitive and interpretable result. Also Silhouette score is less sensitive to the shape of the clusters.\n",
        "Built a Recommendation system that can help Netflix improve user experience and reduce subscriber churn by providing personalized recommendations to users based on their similarity scores."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}